{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v13 10 Oct 2023 \n",
    "# artificial viscosity - achieved with TF variables\n",
    "# made adjustment on IC/BC loss functions\n",
    "\n",
    "## parametric method:\n",
    "# ###### the shock width decreases with time ###### and loss and nu_max trajectory shows reasonable.\n",
    "# However the theta_map and nu_map show random values all over the place without the \"Gaussian distribution\"\n",
    "# Try designate the Gaussian distribution over the viscosity map\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# data = scipy.io.loadmat('burgers_shock.mat')\n",
    "# Exact = data['usol']\n",
    "# Exact_u = np.real(Exact)\n",
    "\n",
    "# Define the domain class\n",
    "class Domain:\n",
    "    def __init__(self, x_min, x_max, t_min, t_max, num_x, num_t, viscosity_init_v):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.t_min = t_min\n",
    "        self.t_max = t_max\n",
    "        self.num_x = num_x\n",
    "        self.num_t = num_t\n",
    "        self.viscosity = tf.Variable(initial_value=tf.ones([]) * viscosity_init_v, \n",
    "                                     trainable=True, \n",
    "                                     dtype=tf.float32,\n",
    "                                     name=\"artificial_viscosity\")\n",
    "        # initial value can be tf.random.uniform(shape=(), minval=0.1, maxval=1.0)\n",
    "        self.theta = tf.Variable(initial_value=tf.ones([256, 1]) * 0.5, \n",
    "                                     trainable=True, \n",
    "                                     dtype=tf.float32,\n",
    "                                     name=\"viscosity_map_coefficient\")\n",
    "    \n",
    "# Define the initial condition\n",
    "def initial_condition(domain, x):\n",
    "    # initial_output = np.where(x > 0, 1, 0)\n",
    "    initial_output = -np.sin(np.pi * x)\n",
    "    return initial_output \n",
    "\n",
    "# Define the boundary conditions\n",
    "def boundary_conditions(domain, x_boundary, x, t):\n",
    "    return np.zeros_like(x)\n",
    "\n",
    "def viscosity_mapping(x, t, theta, epoch):\n",
    "    _nu_map = theta * (x + t) # Calculate the viscosity for each pair of x and t\n",
    "    # Bound the nu values to between 0 and 1\n",
    "    viscosity = domain.viscosity * tf.sigmoid(_nu_map) \n",
    "    storage = [x, t, theta, viscosity, epoch]\n",
    "    \n",
    "    return viscosity, storage #TODO link the x and t to viscosity maps\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "def physics_informed_loss(network, domain, x, t, batch_size, epoch):\n",
    "    # define a viscosity map, displaying the preference of AV on domain\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x)\n",
    "        tape.watch(t)\n",
    "        \n",
    "        u = network(tf.concat([x, t], axis=1))\n",
    "\n",
    "        u_x = tape.gradient(u, x) # cancel: Reshape x to [32,1]\n",
    "        u_t = tape.gradient(u, t) # solved: u_t showed NoneType\n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "\n",
    "        # Debugging: Check if any tensors contain NaN values\n",
    "        tf.debugging.assert_all_finite(u_x, \"u_x contains NaN values\")\n",
    "        tf.debugging.assert_all_finite(u_t, \"u_t contains NaN values\")\n",
    "        \n",
    "        # define the parametric viscosity map with the collocation points in the residual learning batch\n",
    "        # keep record of the parameters: x_coords, t_coords, theta, product of nu_max and nu_map\n",
    "        viscosity_map, parameter_storage = viscosity_mapping(x, t, domain.theta, epoch)\n",
    "\n",
    "        # Define the Burgers equation residual\n",
    "        residual = u_t + u * u_x - viscosity_map * u_xx\n",
    "        # viscosity_map: nu_max * nu_map\n",
    "    \n",
    "    # Define the boundary and initial condition residuals # Done_TODO check \"u\" LR\n",
    "    # Compute the loss for initial condition\n",
    "    num_batch_size_0 = 32\n",
    "    t_batch_0 = tf.fill([num_batch_size_0, 1], 0.001) # considered as 0\n",
    "    x_batch_0 = tf.random.uniform([num_batch_size_0, 1], minval=domain.x_min, maxval=domain.x_max)\n",
    "    u_0 = network(tf.concat([x_batch_0, t_batch_0], axis=1))\n",
    "    initial_residual = u_0 - initial_condition(domain, x_batch_0)\n",
    "\n",
    "    # Compute the loss for boundary condition\n",
    "    num_batch_size_b = 32\n",
    "    x_batch_bl = tf.cast(tf.fill([num_batch_size_b, 1], -1), dtype=tf.float32)\n",
    "    x_batch_br = tf.cast(tf.fill([num_batch_size_b, 1], 1), dtype=tf.float32)\n",
    "    t_batch_b  = tf.random.uniform([num_batch_size_b, 1], minval=domain.t_min, maxval=domain.t_max)\n",
    "    u_bl = network(tf.concat([x_batch_bl, t_batch_b], axis=1)) # int32 and float incompatible\n",
    "    u_br = network(tf.concat([x_batch_br, t_batch_b], axis=1))\n",
    "    lower_boundary_residual = u_bl - boundary_conditions(domain, domain.x_min, x_batch_bl, t_batch_b)\n",
    "    upper_boundary_residual = u_br - boundary_conditions(domain, domain.x_max, x_batch_br, t_batch_b)\n",
    "\n",
    "    viscosity_loss = tf.square(tf.reduce_max(domain.viscosity)) # square the maximum element\n",
    "    loss = tf.reduce_mean(tf.square(residual)) / batch_size + \\\n",
    "           tf.reduce_mean(tf.square(initial_residual)) / num_batch_size_0 + \\\n",
    "           tf.reduce_mean(tf.square(lower_boundary_residual)) / num_batch_size_b + \\\n",
    "           tf.reduce_mean(tf.square(upper_boundary_residual)) / num_batch_size_b + \\\n",
    "            viscosity_loss\n",
    "    \n",
    "    # print(\"viscosity map: \",  domain.viscosity * _viscosity_map) # 256,1\n",
    "    return loss, viscosity_map, parameter_storage\n",
    "\n",
    "# Define the neural network model\n",
    "def create_network():\n",
    "    network = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(2,)),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return network\n",
    "\n",
    "# Train the model using PINNs\n",
    "def train_model(network, domain, num_epochs, learning_rate, batch_size):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Lists to store loss and viscosity values for each epoch\n",
    "    loss_history = []\n",
    "    viscosity_history = []\n",
    "    # theta_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        x_batch = tf.random.uniform([batch_size, 1], minval=domain.x_min, maxval=domain.x_max)\n",
    "        t_batch = tf.random.uniform([batch_size, 1], minval=domain.t_min, maxval=domain.t_max)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss, viscosity_map, parameter_s = physics_informed_loss(network, domain, x_batch, t_batch, batch_size, epoch)\n",
    "\n",
    "        # add viscosity into the trainable variables from TF\n",
    "        variable_list = network.trainable_variables + [domain.viscosity] + \\\n",
    "                        [domain.theta]\n",
    "\n",
    "        gradients = tape.gradient(loss, variable_list)\n",
    "        optimizer.apply_gradients(zip(gradients, variable_list))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # Append the loss and viscosity values to the history lists\n",
    "            loss_history.append(loss.numpy())\n",
    "            viscosity_history.append(tf.reduce_max(domain.viscosity).numpy())\n",
    "            # theta_history.append(domain.theta)\n",
    "    \n",
    "        if epoch % 4999 == 0:\n",
    "            print(\"epoch reached 4999\")\n",
    "            # print(parameter_s)\n",
    "        # if epoch % 5000 == 0: # consider the last epoch\n",
    "            # output the x_batch and t_batch from the PI_loss function\n",
    "            # match the x and t to its velocity map and velocity max\n",
    "\n",
    "    return network, loss_history, viscosity_history, viscosity_map, parameter_s\n",
    "# worked after changing it to \"network\"\n",
    "#TODO: showed error: NameError                        Traceback (most recent call last)\n",
    "# /Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct_v2.ipynb Cell 1 line 1\n",
    "#     139 network = create_network()\n",
    "#     141 # Train the model using PINNs\n",
    "# --> 142 model = train_model(network, domain, num_epochs=2000, learning_rate=0.01, batch_size=32, num_collocation_points=1000)\n",
    "#     144 # Visualize the results\n",
    "#     145 visualize_results(model, domain)\n",
    "# \n",
    "# /Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct_v2.ipynb Cell 1 line 1\n",
    "#     106     if epoch % 100 == 0:\n",
    "#     107         print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "# --> 109 return model\n",
    "# \n",
    "# NameError: name 'model' is not defined\n",
    "\n",
    "# Visualize the results\n",
    "def visualize_results(model, domain, loss_history, viscosity_history, num_epochs):\n",
    "    x_grid, t_grid = np.meshgrid(np.linspace(domain.x_min, domain.x_max, domain.num_x),\n",
    "                                 np.linspace(domain.t_min, domain.t_max, domain.num_t))\n",
    "    X = np.hstack((x_grid.flatten()[:, tf.newaxis], t_grid.flatten()[:, tf.newaxis]))\n",
    "    u_pred_grid = model.predict(X)\n",
    "    u_pred_grid = griddata(X, u_pred_grid.flatten(), (x_grid, t_grid), method='cubic')\n",
    "\n",
    "    print(\"u_pred_shape:\", u_pred_grid.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pcolor(x_grid, t_grid, u_pred_grid, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.title('Predicted Velocity')\n",
    "    plt.show()\n",
    "\n",
    "    # Define the x-axis labels at intervals of every 10 epochs\n",
    "    x_labels = list(range(0, num_epochs, 10))\n",
    "\n",
    "    # Plot the loss and viscosity trajectories\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x_labels, loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Trajectory')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x_labels, viscosity_history)\n",
    "    plt.axhline(y=0.01, color='r', linestyle='--', label='Ground Truth: nu = 0.01')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Viscosity')\n",
    "    plt.title('Viscosity Trajectory')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define domain parameters and create the domain\n",
    "    x_min = -1.0\n",
    "    x_max = 1.0\n",
    "    t_min = 0\n",
    "    t_max = 1.00\n",
    "    num_x = 256\n",
    "    num_t = 100\n",
    "    viscosity_init_v = tf.random.uniform(shape=(), minval=0.001, maxval=1.0)\n",
    "    domain = Domain(x_min, x_max, t_min, t_max, num_x, num_t, viscosity_init_v)\n",
    "    num_epochs = 2000\n",
    "\n",
    "    # Create the neural network \n",
    "    network = create_network()\n",
    "\n",
    "    # Train the model using PINNs\n",
    "    model, loss_history, viscosity_history, viscosity_map, parameter_s = train_model(network, \n",
    "                                                         domain, \n",
    "                                                         num_epochs, \n",
    "                                                         learning_rate=0.01, \n",
    "                                                         batch_size=256\n",
    "                                                         )\n",
    "\n",
    "    # Visualize the results\n",
    "    u_pred_grid = visualize_results(model, \n",
    "                                    domain, \n",
    "                                    loss_history, \n",
    "                                    viscosity_history, \n",
    "                                    num_epochs)\n",
    "\n",
    "    #TODO compare the u_pred_grid with the analytical solution\n",
    "\n",
    "    print(domain.viscosity * viscosity_map)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
