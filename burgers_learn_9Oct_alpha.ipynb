{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<function softplus at 0x13077eb80>) with an unsupported type (<class 'function'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 234\u001b[0m\n\u001b[1;32m    231\u001b[0m network \u001b[39m=\u001b[39m create_network()\n\u001b[1;32m    233\u001b[0m \u001b[39m# Train the model using PINNs\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m model, loss_history, viscosity_history, alpha_history \u001b[39m=\u001b[39m train_model(network, \n\u001b[1;32m    235\u001b[0m                                                      domain, \n\u001b[1;32m    236\u001b[0m                                                      num_epochs, \n\u001b[1;32m    237\u001b[0m                                                      learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, \n\u001b[1;32m    238\u001b[0m                                                      batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m\n\u001b[1;32m    239\u001b[0m                                                      )\n\u001b[1;32m    241\u001b[0m \u001b[39m# Visualize the results\u001b[39;00m\n\u001b[1;32m    242\u001b[0m u_pred_grid \u001b[39m=\u001b[39m visualize_results(model, domain, loss_history, viscosity_history, \n\u001b[1;32m    243\u001b[0m                                 alpha_history, \n\u001b[1;32m    244\u001b[0m                                 num_epochs)\n",
      "Cell \u001b[0;32mIn[6], line 131\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(network, domain, num_epochs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m    128\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, variable_list)\n\u001b[1;32m    129\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, variable_list))\n\u001b[0;32m--> 131\u001b[0m domain\u001b[39m.\u001b[39malpha \u001b[39m=\u001b[39m positive_constraint_fn(domain\u001b[39m.\u001b[39;49malpha) \u001b[39m# keep alpha positive\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m# try#1 Update the viscosity value during training\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m# domain.viscosity.assign(network.layers[-1].weights[0].numpy())\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m# print(\"update.numpy:\", viscosity_update.shape)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m# domain.viscosity.assign_sub(viscosity_update)  # Update the last gradient element\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[6], line 106\u001b[0m, in \u001b[0;36mpositive_constraint_fn\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpositive_constraint_fn\u001b[39m(x):\n\u001b[1;32m    105\u001b[0m     x_value \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m--> 106\u001b[0m     x\u001b[39m.\u001b[39massign(x_value \u001b[39m+\u001b[39m tfp\u001b[39m.\u001b[39;49mutil\u001b[39m.\u001b[39;49mDeferredTensor(tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49msoftplus, x_value))\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow_probability/python/util/deferred_tensor.py:223\u001b[0m, in \u001b[0;36mDeferredTensor.__init__\u001b[0;34m(self, pretransformed_input, transform_fn, dtype, shape, also_track, name)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, pretransformed_input, transform_fn, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    186\u001b[0m              shape\u001b[39m=\u001b[39mNONE_SPECIFIED, also_track\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    187\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates the `DeferredTensor` object.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39m      properties (and `dtype` and/or `shape` arguments are unspecified).\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m   pretransformed_input \u001b[39m=\u001b[39m tensor_util\u001b[39m.\u001b[39;49mconvert_nonref_to_tensor(\n\u001b[1;32m    224\u001b[0m       pretransformed_input,\n\u001b[1;32m    225\u001b[0m       name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpretransformed_input\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    227\u001b[0m   \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     dtype \u001b[39m=\u001b[39m (\u001b[39mgetattr\u001b[39m(transform_fn, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m    229\u001b[0m              dtype_util\u001b[39m.\u001b[39mbase_dtype(pretransformed_input\u001b[39m.\u001b[39mdtype))\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow_probability/python/internal/tensor_util.py:119\u001b[0m, in \u001b[0;36mconvert_nonref_to_tensor\u001b[0;34m(value, dtype, dtype_hint, as_shape_tensor, name)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m as_shape_tensor:\n\u001b[1;32m    117\u001b[0m   \u001b[39mreturn\u001b[39;00m prefer_static\u001b[39m.\u001b[39mconvert_to_shape_tensor(\n\u001b[1;32m    118\u001b[0m       value, dtype\u001b[39m=\u001b[39mdtype, dtype_hint\u001b[39m=\u001b[39mdtype_hint, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[1;32m    120\u001b[0m     value, dtype\u001b[39m=\u001b[39;49mdtype, dtype_hint\u001b[39m=\u001b[39;49mdtype_hint, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (<function softplus at 0x13077eb80>) with an unsupported type (<class 'function'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "# v?? 9 Oct 2023 ## abandoned \n",
    "# artificial viscosity - achieved with TF variables\n",
    "# made adjustment on IC/BC loss functions\n",
    "################ loss_visc = alpha * tf.square(domain.viscosity) ################\n",
    "##TODO might not be feasible since there's a training weight applied to nu by TF anyway\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# data = scipy.io.loadmat('burgers_shock.mat')\n",
    "# Exact = data['usol']\n",
    "# Exact_u = np.real(Exact)\n",
    "\n",
    "# Define the domain class\n",
    "class Domain:\n",
    "    def __init__(self, x_min, x_max, t_min, t_max, num_x, num_t, viscosity_init_v, alpha_init_v):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.t_min = t_min\n",
    "        self.t_max = t_max\n",
    "        self.num_x = num_x\n",
    "        self.num_t = num_t\n",
    "        self.viscosity = tf.Variable(initial_value=viscosity_init_v, \n",
    "                                     trainable=True, \n",
    "                                     dtype=tf.float32,\n",
    "                                     name=\"artificial_viscosity\")\n",
    "        self.alpha = tf.Variable(initial_value=alpha_init_v,\n",
    "                                 trainable=True,\n",
    "                                 dtype=tf.float32,\n",
    "                                 name=\"viscoosity_coefficient\"\n",
    "                                 )\n",
    "        # initial value can be tf.random.uniform(shape=(), minval=0.1, maxval=1.0)\n",
    "    \n",
    "# Define the initial condition\n",
    "def initial_condition(domain, x):\n",
    "    # initial_output = np.where(x > 0, 1, 0)\n",
    "    initial_output = -np.sin(np.pi * x)\n",
    "    return initial_output \n",
    "\n",
    "# Define the boundary conditions\n",
    "def boundary_conditions(domain, x_boundary, x, t):\n",
    "    return np.zeros_like(x)\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "def physics_informed_loss(network, domain, x, t, batch_size):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x)\n",
    "        tape.watch(t)\n",
    "        \n",
    "        u = network(tf.concat([x, t], axis=1))\n",
    "\n",
    "        u_x = tape.gradient(u, x) # cancel: Reshape x to [32,1]\n",
    "        u_t = tape.gradient(u, t) # solved: u_t showed NoneType\n",
    "\n",
    "        # Debugging: Check if any tensors contain NaN values\n",
    "        tf.debugging.assert_all_finite(u_x, \"u_x contains NaN values\")\n",
    "        tf.debugging.assert_all_finite(u_t, \"u_t contains NaN values\")\n",
    "\n",
    "        # Define the Burgers equation residual\n",
    "        residual = u_t + u * u_x - domain.viscosity / np.pi * tape.gradient(u_x, x)\n",
    "    \n",
    "    # Define the boundary and initial condition residuals # Done_TODO check \"u\" LR\n",
    "    # Compute the loss for initial condition\n",
    "    num_batch_size_0 = 32\n",
    "    t_batch_0 = tf.fill([num_batch_size_0, 1], 0.001) # considered as 0\n",
    "    x_batch_0 = tf.random.uniform([num_batch_size_0, 1], minval=domain.x_min, maxval=domain.x_max)\n",
    "    u_0 = network(tf.concat([x_batch_0, t_batch_0], axis=1))\n",
    "    initial_residual = u_0 - initial_condition(domain, x_batch_0)\n",
    "\n",
    "    # Compute the loss for boundary condition\n",
    "    num_batch_size_b = 32\n",
    "    x_batch_bl = tf.cast(tf.fill([num_batch_size_b, 1], -1), dtype=tf.float32)\n",
    "    x_batch_br = tf.cast(tf.fill([num_batch_size_b, 1], 1), dtype=tf.float32)\n",
    "    t_batch_b  = tf.random.uniform([num_batch_size_b, 1], minval=domain.t_min, maxval=domain.t_max)\n",
    "    u_bl = network(tf.concat([x_batch_bl, t_batch_b], axis=1)) # int32 and float incompatible\n",
    "    u_br = network(tf.concat([x_batch_br, t_batch_b], axis=1))\n",
    "    lower_boundary_residual = u_bl - boundary_conditions(domain, domain.x_min, x_batch_bl, t_batch_b)\n",
    "    upper_boundary_residual = u_br - boundary_conditions(domain, domain.x_max, x_batch_br, t_batch_b)\n",
    "\n",
    "    viscosity_loss = domain.alpha * tf.square(domain.viscosity) \n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(residual)) / batch_size + \\\n",
    "           tf.reduce_mean(tf.square(initial_residual)) / num_batch_size_0 + \\\n",
    "           tf.reduce_mean(tf.square(lower_boundary_residual)) / num_batch_size_b + \\\n",
    "           tf.reduce_mean(tf.square(upper_boundary_residual)) / num_batch_size_b + \\\n",
    "            viscosity_loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Define the neural network model\n",
    "def create_network():\n",
    "    network = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(2,)),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return network\n",
    "\n",
    "# Apply a positivity constraint using TensorFlow Probability\n",
    "def positive_constraint_fn(x):\n",
    "    x_value = x.numpy()\n",
    "    x.assign(x_value + tfp.util.DeferredTensor(tf.nn.softplus, x_value))\n",
    "    return x\n",
    "\n",
    "# Train the model using PINNs\n",
    "def train_model(network, domain, num_epochs, learning_rate, batch_size):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Lists to store loss and viscosity values for each epoch\n",
    "    loss_history = []\n",
    "    viscosity_history = []\n",
    "    alpha_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        x_batch = tf.random.uniform([batch_size, 1], minval=domain.x_min, maxval=domain.x_max)\n",
    "        t_batch = tf.random.uniform([batch_size, 1], minval=domain.t_min, maxval=domain.t_max)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = physics_informed_loss(network, domain, x_batch, t_batch, batch_size)\n",
    "\n",
    "        # add viscosity into the trainable variables from TF\n",
    "        variable_list = network.trainable_variables + [domain.viscosity] + [domain.alpha]\n",
    "\n",
    "        gradients = tape.gradient(loss, variable_list)\n",
    "        optimizer.apply_gradients(zip(gradients, variable_list))\n",
    "\n",
    "        domain.alpha = positive_constraint_fn(domain.alpha) # keep alpha positive\n",
    "    \n",
    "        # try#1 Update the viscosity value during training\n",
    "        # domain.viscosity.assign(network.layers[-1].weights[0].numpy())\n",
    "\n",
    "        # try#2\n",
    "        # Manually update the trainable_viscosity variable based on gradients\n",
    "        # viscosity_update = tf.reduce_sum(learning_rate * gradients[-1])\n",
    "        # print(\"shape of nu:\", domain.viscosity)\n",
    "        # print(\"update.numpy:\", viscosity_update.shape)\n",
    "        # domain.viscosity.assign_sub(viscosity_update)  # Update the last gradient element\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # Append the loss and viscosity values to the history lists\n",
    "            loss_history.append(loss.numpy())\n",
    "            viscosity_history.append(domain.viscosity.numpy())\n",
    "            alpha_history.append(domain.alpha.numpy())\n",
    "    \n",
    "    return network, loss_history, viscosity_history, alpha_history\n",
    "# worked after changing it to \"network\"\n",
    "#TODO: showed error: NameError                        Traceback (most recent call last)\n",
    "# /Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct_v2.ipynb Cell 1 line 1\n",
    "#     139 network = create_network()\n",
    "#     141 # Train the model using PINNs\n",
    "# --> 142 model = train_model(network, domain, num_epochs=2000, learning_rate=0.01, batch_size=32, num_collocation_points=1000)\n",
    "#     144 # Visualize the results\n",
    "#     145 visualize_results(model, domain)\n",
    "# \n",
    "# /Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct_v2.ipynb Cell 1 line 1\n",
    "#     106     if epoch % 100 == 0:\n",
    "#     107         print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "# --> 109 return model\n",
    "# \n",
    "# NameError: name 'model' is not defined\n",
    "\n",
    "# Visualize the results\n",
    "def visualize_results(model, domain, loss_history, viscosity_history, alpha_history, num_epochs):\n",
    "    x_grid, t_grid = np.meshgrid(np.linspace(domain.x_min, domain.x_max, domain.num_x),\n",
    "                                 np.linspace(domain.t_min, domain.t_max, domain.num_t))\n",
    "    X = np.hstack((x_grid.flatten()[:, tf.newaxis], t_grid.flatten()[:, tf.newaxis]))\n",
    "    u_pred_grid = model.predict(X)\n",
    "    u_pred_grid = griddata(X, u_pred_grid.flatten(), (x_grid, t_grid), method='cubic')\n",
    "\n",
    "    print(\"u_pred_shape:\", u_pred_grid.shape)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pcolor(x_grid, t_grid, u_pred_grid, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.title('Predicted Velocity')\n",
    "    plt.show()\n",
    "\n",
    "    # Define the x-axis labels at intervals of every 10 epochs\n",
    "    x_labels = list(range(0, num_epochs, 10))\n",
    "\n",
    "    # Plot the loss and viscosity trajectories\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(x_labels, loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Trajectory')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(x_labels, viscosity_history)\n",
    "    plt.axhline(y=0.01, color='r', linestyle='--', label='Ground Truth: nu = 0.01')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Viscosity')\n",
    "    plt.title('Viscosity Trajectory')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(x_labels, alpha_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Alpha')\n",
    "    plt.title('Alpha Trajectory')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return u_pred_grid\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define domain parameters and create the domain\n",
    "    x_min = -1.0\n",
    "    x_max = 1.0\n",
    "    t_min = 0\n",
    "    t_max = 1.00\n",
    "    num_x = 256\n",
    "    num_t = 100\n",
    "    viscosity_init_v = tf.random.uniform(shape=(), minval=0.001, maxval=1.0)\n",
    "    alpha_init_v = tf.random.uniform(shape=(), minval=0.0001, maxval=0.001)\n",
    "    domain = Domain(x_min, x_max, t_min, t_max, num_x, num_t, viscosity_init_v, alpha_init_v)\n",
    "    num_epochs = 2000\n",
    "\n",
    "    # Create the neural network \n",
    "    network = create_network()\n",
    "\n",
    "    # Train the model using PINNs\n",
    "    model, loss_history, viscosity_history, alpha_history = train_model(network, \n",
    "                                                         domain, \n",
    "                                                         num_epochs, \n",
    "                                                         learning_rate=0.01, \n",
    "                                                         batch_size=256\n",
    "                                                         )\n",
    "\n",
    "    # Visualize the results\n",
    "    u_pred_grid = visualize_results(model, domain, loss_history, viscosity_history, \n",
    "                                    alpha_history, \n",
    "                                    num_epochs)\n",
    "\n",
    "    print(domain.viscosity.numpy())\n",
    "    print(domain.alpha.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
