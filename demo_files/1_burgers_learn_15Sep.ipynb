{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_x tf.Tensor(\n",
      "[[0.35362712]\n",
      " [0.34899762]\n",
      " [0.33561212]\n",
      " [0.3459878 ]\n",
      " [0.35398376]\n",
      " [0.3523803 ]\n",
      " [0.3553429 ]\n",
      " [0.32603762]\n",
      " [0.32496783]\n",
      " [0.34658563]\n",
      " [0.33789608]\n",
      " [0.3249273 ]\n",
      " [0.34429562]\n",
      " [0.34345073]\n",
      " [0.35193306]\n",
      " [0.3564657 ]\n",
      " [0.34596062]\n",
      " [0.35375854]\n",
      " [0.32730955]\n",
      " [0.35665745]\n",
      " [0.35456026]\n",
      " [0.32884908]\n",
      " [0.3572298 ]\n",
      " [0.32768416]\n",
      " [0.31209248]\n",
      " [0.34915388]\n",
      " [0.32192713]\n",
      " [0.35716164]\n",
      " [0.35686296]\n",
      " [0.3386773 ]\n",
      " [0.35589674]\n",
      " [0.33904314]], shape=(32, 1), dtype=float32)\n",
      "u_t None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m model \u001b[39m=\u001b[39m create_model()\n\u001b[1;32m    113\u001b[0m \u001b[39m# Train the model using PINNs\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m model \u001b[39m=\u001b[39m train_model(model, domain, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[1;32m    116\u001b[0m \u001b[39m# Visualize the results\u001b[39;00m\n\u001b[1;32m    117\u001b[0m visualize_results(model, domain)\n",
      "Cell \u001b[0;32mIn[7], line 73\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, domain, num_epochs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m t_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform([batch_size, \u001b[39m1\u001b[39m], minval\u001b[39m=\u001b[39mdomain\u001b[39m.\u001b[39mt_min, maxval\u001b[39m=\u001b[39mdomain\u001b[39m.\u001b[39mt_max)\n\u001b[1;32m     72\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m---> 73\u001b[0m     loss \u001b[39m=\u001b[39m physics_informed_loss(model, domain, x_batch, t_batch)\n\u001b[1;32m     75\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, model\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[1;32m     76\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, model\u001b[39m.\u001b[39mtrainable_variables))\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mphysics_informed_loss\u001b[0;34m(model, domain, x, t)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mu_t\u001b[39m\u001b[39m\"\u001b[39m, u_t)\n\u001b[1;32m     38\u001b[0m \u001b[39m# Define the Burgers equation residual\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m residual \u001b[39m=\u001b[39m u_t \u001b[39m+\u001b[39;49m u \u001b[39m*\u001b[39;49m u_x \u001b[39m-\u001b[39m domain\u001b[39m.\u001b[39mviscosity \u001b[39m*\u001b[39m tape\u001b[39m.\u001b[39mgradient(u_x, x)\n\u001b[1;32m     40\u001b[0m \u001b[39m# this is going to transform variables to a Tensor\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m# Define the boundary and initial condition residuals\u001b[39;00m\n\u001b[1;32m     43\u001b[0m initial_residual \u001b[39m=\u001b[39m u \u001b[39m-\u001b[39m initial_condition(domain, x)\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Define the domain class\n",
    "class Domain:\n",
    "    def __init__(self, x_min, x_max, t_min, t_max, num_x, num_t, viscosity):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.t_min = t_min\n",
    "        self.t_max = t_max\n",
    "        self.num_x = num_x\n",
    "        self.num_t = num_t\n",
    "        self.viscosity = viscosity\n",
    "\n",
    "# Define the initial condition\n",
    "def initial_condition(domain, x):\n",
    "    return -np.sin(np.pi * x)\n",
    "\n",
    "# Define the boundary conditions\n",
    "def boundary_conditions(domain, x_boundary, x, t):\n",
    "    return np.zeros_like(x)\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "def physics_informed_loss(model, domain, x, t):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x)\n",
    "        tape.watch(t)\n",
    "\n",
    "        u = model(x, t)\n",
    "        u_x = tape.gradient(u, x)\n",
    "        u_t = tape.gradient(u, t)\n",
    "\n",
    "        print(\"u_x\", u_x)\n",
    "        print(\"u_t\", u_t)\n",
    "\n",
    "        # Define the Burgers equation residual\n",
    "        residual = u_t + u * u_x - domain.viscosity * tape.gradient(u_x, x)\n",
    "        # this is going to transform variables to a Tensor\n",
    "\n",
    "        # Define the boundary and initial condition residuals\n",
    "        initial_residual = u - initial_condition(domain, x)\n",
    "        lower_boundary_residual = u - boundary_conditions(domain, domain.x_min, x, t)\n",
    "        upper_boundary_residual = u - boundary_conditions(domain, domain.x_max, x, t)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(residual)) + \\\n",
    "           tf.reduce_mean(tf.square(initial_residual)) + \\\n",
    "           tf.reduce_mean(tf.square(lower_boundary_residual)) + \\\n",
    "           tf.reduce_mean(tf.square(upper_boundary_residual))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Define the neural network model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,)),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Train the model using PINNs\n",
    "def train_model(model, domain, num_epochs, learning_rate, batch_size):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        x_batch = tf.random.uniform([batch_size, 1], minval=domain.x_min, maxval=domain.x_max)\n",
    "        t_batch = tf.random.uniform([batch_size, 1], minval=domain.t_min, maxval=domain.t_max)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = physics_informed_loss(model, domain, x_batch, t_batch)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Visualize the results\n",
    "def visualize_results(model, domain):\n",
    "    x_grid, t_grid = np.meshgrid(np.linspace(domain.x_min, domain.x_max, domain.num_x),\n",
    "                                 np.linspace(domain.t_min, domain.t_max, domain.num_t))\n",
    "    X = np.hstack((x_grid.flatten()[:, np.newaxis], t_grid.flatten()[:, np.newaxis]))\n",
    "    u_pred_grid = model.predict(X)\n",
    "    u_pred_grid = griddata(X, u_pred_grid.flatten(), (x_grid, t_grid), method='cubic')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pcolor(x_grid, t_grid, u_pred_grid, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.title('Predicted Velocity')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define domain parameters and create the domain\n",
    "    x_min = -1.0\n",
    "    x_max = 1.0\n",
    "    t_min = 0.0\n",
    "    t_max = 0.99\n",
    "    num_x = 100\n",
    "    num_t = 100\n",
    "    viscosity = 0.01\n",
    "    domain = Domain(x_min, x_max, t_min, t_max, num_x, num_t, viscosity)\n",
    "\n",
    "    # Create the neural network \n",
    "    model = create_model()\n",
    "\n",
    "    # Train the model using PINNs\n",
    "    model = train_model(model, domain, num_epochs=2000, learning_rate=0.01, batch_size=32)\n",
    "\n",
    "    # Visualize the results\n",
    "    visualize_results(model, domain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avvenv",
   "language": "python",
   "name": "avvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, May  7 2023, 23:32:44) \n[Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
