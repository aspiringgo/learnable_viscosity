{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2)\n",
      "u: Tensor(\"sequential_21/dense_65/BiasAdd:0\", shape=(32, 1), dtype=float32)\n",
      "x_v: <tf.Variable 'Variable_20:0' shape=(32, 1) dtype=float32>\n",
      "t_v: <tf.Variable 'Variable_21:0' shape=(32, 1) dtype=float32>\n",
      "shape of x (32, 1)\n",
      "shape of u (32, 1)\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_x Tensor(\"gradient_tape/Slice_42:0\", shape=(32, 1), dtype=float32)\n",
      "u_t Tensor(\"gradient_tape/Slice_45:0\", shape=(32, 1), dtype=float32)\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "2023-10-03 14:45:46.167613: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual: [[ 0.06465168]\n",
      " [-0.03463315]\n",
      " [-0.16685446]\n",
      " [-0.1678846 ]\n",
      " [-0.135828  ]\n",
      " [-0.04430117]\n",
      " [-0.16778208]\n",
      " [-0.03608149]\n",
      " [-0.03125649]\n",
      " [-0.14736545]\n",
      " [ 0.00357426]\n",
      " [-0.00336998]\n",
      " [-0.19067186]\n",
      " [-0.02188469]\n",
      " [ 0.01569228]\n",
      " [ 0.00807067]\n",
      " [-0.10196745]\n",
      " [-0.08291439]\n",
      " [-0.1537114 ]\n",
      " [-0.17444548]\n",
      " [-0.03091907]\n",
      " [ 0.03993639]\n",
      " [ 0.02494384]\n",
      " [ 0.00418992]\n",
      " [ 0.02506239]\n",
      " [-0.0114854 ]\n",
      " [ 0.01635067]\n",
      " [-0.10042171]\n",
      " [ 0.01327264]\n",
      " [-0.07496114]\n",
      " [-0.15653017]\n",
      " [ 0.05973197]]\n"
     ]
    }
   ],
   "source": [
    "# final\n",
    "# examine the input shape #5 Variables\n",
    "import tensorflow as tf\n",
    "# tf.compat.v1.disable_eager_execution() \n",
    "# TODO: don't know it this works\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_v = tf.Variable(tf.random.uniform([32, 1], minval=-1,\n",
    "    maxval=1), dtype=tf.float32)\n",
    "t_v = tf.Variable(tf.random.uniform([32, 1], minval=0,\n",
    "    maxval=0.99), dtype=tf.float32)\n",
    "# input_v = tf.concat([x_v, t_v], axis=1)\n",
    "# print(input_v.shape)\n",
    "\n",
    "def create_network():\n",
    "    network = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(2,)),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return network\n",
    "network= create_network()\n",
    "\n",
    "#### watch ####\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "        #x0 = input_v[:,0]\n",
    "        #x = x0[:, tf.newaxis] # reshape x to [32,1]\n",
    "        #t0 = input_v[:,1]\n",
    "        #t = t0[:, tf.newaxis] # reshape t to [32,1]\n",
    "        u = network(tf.concat([x_v, t_v], axis=1))\n",
    "        tape.watch(x_v)\n",
    "        tape.watch(t_v)\n",
    "        #tape.watch(u)\n",
    "\n",
    "        print(\"u:\",u)\n",
    "        print(\"x_v:\", x_v)\n",
    "        print(\"t_v:\", t_v)\n",
    "        print(\"shape of x\", x_v.shape)\n",
    "        print(\"shape of u\", u.shape)\n",
    "\n",
    "        u_x = tape.gradient(u, x_v)\n",
    "        u_t = tape.gradient(u, t_v)\n",
    "\n",
    "        print(\"u_x\", u_x)\n",
    "        print(\"u_t\", u_t)\n",
    "\n",
    "        # Debugging: Check if any tensors contain NaN values\n",
    "        tf.debugging.assert_all_finite(u_x, \"u_x contains NaN values\")\n",
    "        tf.debugging.assert_all_finite(u_t, \"u_t contains NaN values\")\n",
    "\n",
    "        # Define the Burgers equation residual\n",
    "        residual = u_t + u * u_x - 0.01 * tape.gradient(u_x, x_v)\n",
    "# Create a TensorFlow session and evaluate the residual\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    residual_value = sess.run(residual)\n",
    "\n",
    "print(\"residual:\", residual_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2)\n",
      "u_x [None]\n",
      "u_t [None]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected float32 passed to parameter 'y' of op 'Mul', got [None] of type 'list' instead. Error: Expected float32, but got None of type 'NoneType'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mu_t\u001b[39m\u001b[39m\"\u001b[39m, u_t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Define the Burgers equation residual\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m residual \u001b[39m=\u001b[39m u_t \u001b[39m+\u001b[39m u \u001b[39m*\u001b[39;49m u_x \u001b[39m-\u001b[39m \u001b[39m0.01\u001b[39m \u001b[39m*\u001b[39m u_xx\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:561\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    560\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected \u001b[39m\u001b[39m{\u001b[39;00mdtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m passed to parameter \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00minput_arg\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m of op \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mop_type_name\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(values)\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(values)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m instead. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: \u001b[39m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    566\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m   \u001b[39m# What type does convert_to_tensor think it has?\u001b[39;00m\n\u001b[1;32m    568\u001b[0m   \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected float32 passed to parameter 'y' of op 'Mul', got [None] of type 'list' instead. Error: Expected float32, but got None of type 'NoneType'."
     ]
    }
   ],
   "source": [
    "# Examine the input shape #4 Variables\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "x_v = tf.Variable(tf.random.uniform([32, 1], minval=-1,\n",
    "    maxval=1), dtype=tf.float32)\n",
    "t_v = tf.Variable(tf.random.uniform([32, 1], minval=0,\n",
    "    maxval=0.99), dtype=tf.float32)\n",
    "input_v = tf.concat([x_v, t_v], axis=1)\n",
    "print(input_v.shape)\n",
    "\n",
    "def create_network():\n",
    "    network = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(2,)),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return network\n",
    "network= create_network()\n",
    "\n",
    "x0 = input_v[:,0]\n",
    "x = x0[:, tf.newaxis] # reshape x to [32,1]\n",
    "t0 = input_v[:,1]\n",
    "t = t0[:, tf.newaxis] # reshape t to [32,1]\n",
    "u = network(input_v)\n",
    "\n",
    "u_x = tf.gradients(u, x) \n",
    "u_t = tf.gradients(u, t)\n",
    "#u_xx = tf.gradients(u_x, x)\n",
    "\n",
    "print(\"u_x\", u_x)\n",
    "print(\"u_t\", u_t)\n",
    "# Define the Burgers equation residual\n",
    "residual = u_t + u * u_x - 0.01 * u_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 2)\n",
      "u: Tensor(\"sequential_16/dense_50/BiasAdd:0\", shape=(32, 1), dtype=float32)\n",
      "x: Tensor(\"strided_slice_21:0\", shape=(32, 1), dtype=float32)\n",
      "t: Tensor(\"strided_slice_23:0\", shape=(32, 1), dtype=float32)\n",
      "shape of x (32, 1)\n",
      "shape of u (32, 1)\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_x <tensorflow.python.eager.backprop.GradientTape object at 0x150642a60>\n",
      "u_t <tensorflow.python.eager.backprop.GradientTape object at 0x150643610>\n",
      "u_x1 None\n",
      "u_t1 None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed to convert elements of <tensorflow.python.eager.backprop.GradientTape object at 0x150642a60> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W0sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mu_t1\u001b[39m\u001b[39m\"\u001b[39m, u_t1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W0sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Debugging: Check if any tensors contain NaN values\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W0sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m tf\u001b[39m.\u001b[39;49mdebugging\u001b[39m.\u001b[39;49massert_all_finite(u_x, \u001b[39m\"\u001b[39;49m\u001b[39mu_x contains NaN values\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W0sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m tf\u001b[39m.\u001b[39mdebugging\u001b[39m.\u001b[39massert_all_finite(u_t, \u001b[39m\"\u001b[39m\u001b[39mu_t contains NaN values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W0sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Define the Burgers equation residual\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/tensor_util.py:609\u001b[0m, in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    607\u001b[0m   str_values \u001b[39m=\u001b[39m [compat\u001b[39m.\u001b[39mas_bytes(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m proto_values]\n\u001b[1;32m    608\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to convert elements of \u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m}\u001b[39;00m\u001b[39m to Tensor. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mConsider casting elements to a supported type. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    611\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mhttps://www.tensorflow.org/api_docs/python/tf/dtypes \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mfor supported TF dtypes.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    613\u001b[0m tensor_proto\u001b[39m.\u001b[39mstring_val\u001b[39m.\u001b[39mextend(str_values)\n\u001b[1;32m    614\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_proto\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert elements of <tensorflow.python.eager.backprop.GradientTape object at 0x150642a60> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes."
     ]
    }
   ],
   "source": [
    "# Examine the input shape #3 Variables\n",
    "import tensorflow as tf\n",
    "\n",
    "x_v = tf.Variable(tf.random.uniform([32, 1], minval=-1,\n",
    "    maxval=1), dtype=tf.float32)\n",
    "t_v = tf.Variable(tf.random.uniform([32, 1], minval=0,\n",
    "    maxval=0.99), dtype=tf.float32)\n",
    "input_v = tf.concat([x_v, t_v], axis=1)\n",
    "print(input_v.shape)\n",
    "\n",
    "def create_network():\n",
    "    network = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(2,)),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return network\n",
    "network= create_network()\n",
    "\n",
    "#### watch ####\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "        x0 = input_v[:,0]\n",
    "        x = x0[:, tf.newaxis] # reshape x to [32,1]\n",
    "        t0 = input_v[:,1]\n",
    "        t = t0[:, tf.newaxis] # reshape t to [32,1]\n",
    "        u = network(input_v)\n",
    "        tape.watch(x)\n",
    "        tape.watch(t)\n",
    "        tape.watch(u)\n",
    "\n",
    "        print(\"u:\",u)\n",
    "        print(\"x:\", x)\n",
    "        print(\"t:\", t)\n",
    "        print(\"shape of x\", x.shape)\n",
    "        print(\"shape of u\", u.shape)\n",
    "\n",
    "        u_x1 = tape.gradient(u, x)\n",
    "        u_t1 = tape.gradient(u, t)\n",
    "\n",
    "        u_x = tf.GradientTape(u, x) \n",
    "        u_t = tf.GradientTape(u, t)\n",
    "\n",
    "        print(\"u_x\", u_x)\n",
    "        print(\"u_t\", u_t)\n",
    "\n",
    "        print(\"u_x1\", u_x1)\n",
    "        print(\"u_t1\", u_t1)\n",
    "\n",
    "        # Debugging: Check if any tensors contain NaN values\n",
    "        tf.debugging.assert_all_finite(u_x, \"u_x contains NaN values\")\n",
    "        tf.debugging.assert_all_finite(u_t, \"u_t contains NaN values\")\n",
    "\n",
    "        # Define the Burgers equation residual\n",
    "        residual = u_t + u * u_x - 0.01 * tape.gradient(u_x, x[:, tf.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: tf.Tensor(\n",
      "[[-0.18582189]\n",
      " [ 0.24023455]\n",
      " [ 0.01840072]\n",
      " [ 0.2222786 ]\n",
      " [ 0.23959512]\n",
      " [-0.06764428]\n",
      " [ 0.21418434]\n",
      " [ 0.00327495]\n",
      " [ 0.37254554]\n",
      " [ 0.30541083]\n",
      " [ 0.09076623]\n",
      " [ 0.06332825]\n",
      " [ 0.3420956 ]\n",
      " [-0.2254313 ]\n",
      " [ 0.14959903]\n",
      " [-0.31151855]\n",
      " [ 0.33738238]\n",
      " [ 0.32531762]\n",
      " [ 0.13612786]\n",
      " [-0.2617503 ]\n",
      " [ 0.31775922]\n",
      " [ 0.29750836]\n",
      " [ 0.19459987]\n",
      " [ 0.08700581]\n",
      " [-0.00678556]\n",
      " [-0.3216837 ]\n",
      " [-0.33125502]\n",
      " [ 0.04384768]\n",
      " [-0.20862512]\n",
      " [-0.211461  ]\n",
      " [ 0.30239332]\n",
      " [-0.20432311]], shape=(32, 1), dtype=float32)\n",
      "x tf.Tensor(\n",
      "[[ 0.6948271 ]\n",
      " [-0.57448053]\n",
      " [ 0.00312781]\n",
      " [-0.5246    ]\n",
      " [-0.5715091 ]\n",
      " [ 0.3221066 ]\n",
      " [-0.5305848 ]\n",
      " [ 0.09822297]\n",
      " [-0.9713428 ]\n",
      " [-0.7677369 ]\n",
      " [-0.18217945]\n",
      " [-0.12710404]\n",
      " [-0.89247775]\n",
      " [ 0.65450287]\n",
      " [-0.3164158 ]\n",
      " [ 0.87286353]\n",
      " [-0.8733046 ]\n",
      " [-0.82467055]\n",
      " [-0.2872188 ]\n",
      " [ 0.8255148 ]\n",
      " [-0.80380845]\n",
      " [-0.7408223 ]\n",
      " [-0.44552732]\n",
      " [-0.1598041 ]\n",
      " [ 0.11837983]\n",
      " [ 0.91276526]\n",
      " [ 0.92259884]\n",
      " [-0.05844045]\n",
      " [ 0.67445755]\n",
      " [ 0.71848536]\n",
      " [-0.76780176]\n",
      " [ 0.55666137]], shape=(32, 1), dtype=float32)\n",
      "t tf.Tensor(\n",
      "[0.93475085 0.5288857  0.36015663 0.52092636 0.7999279  0.8959304\n",
      " 0.17838064 0.774359   0.7222361  0.37576264 0.38328317 0.2487956\n",
      " 0.92181796 0.35487843 0.6811725  0.21187474 0.882245   0.73342836\n",
      " 0.55108875 0.59098446 0.37739858 0.74051166 0.57516366 0.50326276\n",
      " 0.7151081  0.24822298 0.16714586 0.39536953 0.62596416 0.7489584\n",
      " 0.9201343  0.16680185], shape=(32,), dtype=float32)\n",
      "(32,)\n",
      "(32, 1)\n",
      "u_x <tensorflow.python.eager.backprop.GradientTape object at 0x28820cd60>\n",
      "u_t <tensorflow.python.eager.backprop.GradientTape object at 0x28833b430>\n"
     ]
    }
   ],
   "source": [
    "# Examine the input shape #2\n",
    "x_batch = tf.random.uniform([32, 1], minval=-1, maxval=1)\n",
    "t_batch = tf.random.uniform([32, 1], minval=0, maxval=0.99)\n",
    "\n",
    "input_batch = tf.concat([x_batch, t_batch], axis=1)\n",
    "\n",
    "\n",
    "def create_network():\n",
    "    network = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(2,)),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return network\n",
    "network= create_network()\n",
    "\n",
    "x = input_batch[:,0]\n",
    "t = input_batch[:,1]\n",
    "u = network(input_batch)\n",
    "print(\"u:\",u)\n",
    "\n",
    "print(\"x\", x[:, tf.newaxis])\n",
    "print(\"t\", t)\n",
    "print(x.shape)\n",
    "print(u.shape)\n",
    "\n",
    "u_x = tf.GradientTape(u, x[:, tf.newaxis]) # Reshape x to [32,1]\n",
    "u_t = tf.GradientTape(u, t[:, tf.newaxis]) # u_t showed NoneType\n",
    "\n",
    "print(\"u_x\", u_x)\n",
    "print(\"u_t\", u_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2841267628.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[45], line 38\u001b[0;36m\u001b[0m\n\u001b[0;31m    tape.watch(x)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Examine the input shape #1\n",
    "x_batch = tf.random.uniform([32, 1], minval=-1, maxval=1)\n",
    "t_batch = tf.random.uniform([32, 1], minval=0, maxval=0.99)\n",
    "\n",
    "input_batch = tf.concat([x_batch, t_batch], axis=1)\n",
    "\n",
    "\n",
    "def create_network():\n",
    "    network = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(2,)),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return network\n",
    "network= create_network()\n",
    "\n",
    "x = input_batch[:,0]\n",
    "t = input_batch[:,1]\n",
    "u = network(input_batch)\n",
    "print(\"u:\",u)\n",
    "\n",
    "print(\"x\", x[:, tf.newaxis])\n",
    "print(\"t\", t)\n",
    "print(x.shape)\n",
    "print(u.shape)\n",
    "\n",
    "u_x = tf.gradients(u, x[:, tf.newaxis]) # Reshape x to [32,1]\n",
    "u_t = tf.gradients(u, t[:, tf.newaxis]) # u_t showed NoneType\n",
    "\n",
    "print(\"u_x\", u_x)\n",
    "print(\"u_t\", u_t)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "        x = input_batch[:,0]\n",
    "        t = input_batch[:,1]\n",
    "        \n",
    "        tape.watch(x)\n",
    "        tape.watch(t)\n",
    "        tape.watch(input_batch)\n",
    "\n",
    "        u = network(input_batch)\n",
    "        print(\"u:\",u)\n",
    "\n",
    "        print(\"x\", x[:, tf.newaxis])\n",
    "        print(\"t\", t)\n",
    "        print(x.shape)\n",
    "        print(u.shape)\n",
    "\n",
    "        u_x = tf.gradients(u, x[:, tf.newaxis]) # Reshape x to [32,1]\n",
    "        u_t = tape.gradient(u, t) # u_t showed NoneType\n",
    "\n",
    "        print(\"u_x\", u_x)\n",
    "        print(\"u_t\", u_t)\n",
    "\n",
    "        # Debugging: Check if any tensors contain NaN values\n",
    "        tf.debugging.assert_all_finite(u_x, \"u_x contains NaN values\")\n",
    "        tf.debugging.assert_all_finite(u_t, \"u_t contains NaN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: Tensor(\"sequential_22/dense_68/BiasAdd:0\", shape=(32, 1), dtype=float32)\n",
      "u_shape: (32, 1)\n",
      "x: Tensor(\"random_uniform_44:0\", shape=(32, 1), dtype=float32)\n",
      "x_reshape: (32, 1)\n",
      "t: Tensor(\"random_uniform_45/Mul:0\", shape=(32, 1), dtype=float32)\n",
      "t_reshape: (32, 1)\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_x: Tensor(\"gradient_tape/Slice_48:0\", shape=(32, 1), dtype=float32)\n",
      "u_t: Tensor(\"gradient_tape/Slice_51:0\", shape=(32, 1), dtype=float32)\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u: Tensor(\"sequential_22/dense_68/BiasAdd:0\", shape=(32, 1), dtype=float32)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot convert a symbolic tf.Tensor (mul_23:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m network \u001b[39m=\u001b[39m create_network()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m \u001b[39m# Train the model using PINNs\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(network, domain, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, num_collocation_points\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39m# Visualize the results\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=144'>145</a>\u001b[0m visualize_results(model, domain)\n",
      "\u001b[1;32m/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39m# Concatenate x_batch and t_batch to create input with shape (batch_size, 2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39m# input_batch = tf.concat([x_batch, t_batch], axis=1)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     loss \u001b[39m=\u001b[39m physics_informed_loss(network, domain, x_batch, t_batch, collocation_points)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m gradients \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, network\u001b[39m.\u001b[39mtrainable_variables)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m optimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(gradients, network\u001b[39m.\u001b[39mtrainable_variables))\n",
      "\u001b[1;32m/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Define the boundary and initial condition residuals\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# preliminary check\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mu:\u001b[39m\u001b[39m\"\u001b[39m, u)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m initial_residual \u001b[39m=\u001b[39m u \u001b[39m-\u001b[39m initial_condition(domain, x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m lower_boundary_residual \u001b[39m=\u001b[39m u \u001b[39m-\u001b[39m boundary_conditions(domain, domain\u001b[39m.\u001b[39mx_min, x, t)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m upper_boundary_residual \u001b[39m=\u001b[39m u \u001b[39m-\u001b[39m boundary_conditions(domain, domain\u001b[39m.\u001b[39mx_max, x, t)\n",
      "\u001b[1;32m/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minitial_condition\u001b[39m(domain, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/e0919678/Library/CloudStorage/OneDrive-NationalUniversityofSingapore/Desktop/MEng_files/CFD_HPC/Code_TensorDiffEq/burgers_learn_2Oct.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39;49msin(np\u001b[39m.\u001b[39;49mpi \u001b[39m*\u001b[39;49m x)\n",
      "File \u001b[0;32m~/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/tensor.py:628\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    627\u001b[0m   \u001b[39mdel\u001b[39;00m dtype\n\u001b[0;32m--> 628\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    629\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot convert a symbolic tf.Tensor (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m) to a numpy array.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    630\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m This error may indicate that you\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre trying to pass a Tensor to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    631\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m a NumPy call, which is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic tf.Tensor (mul_23:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported."
     ]
    }
   ],
   "source": [
    "# v3\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Define the domain class\n",
    "class Domain:\n",
    "    def __init__(self, x_min, x_max, t_min, t_max, num_x, num_t, viscosity):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.t_min = t_min\n",
    "        self.t_max = t_max\n",
    "        self.num_x = num_x\n",
    "        self.num_t = num_t\n",
    "        self.viscosity = viscosity\n",
    "\n",
    "# Define the initial condition\n",
    "def initial_condition(domain, x):\n",
    "    return -np.sin(np.pi * x)\n",
    "\n",
    "# Define the boundary conditions\n",
    "def boundary_conditions(domain, x_boundary, x, t):\n",
    "    return np.zeros_like(x)\n",
    "\n",
    "# Define the physics-informed loss function\n",
    "def physics_informed_loss(network, domain, x, t, collocation_points):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(x)\n",
    "        tape.watch(t)\n",
    "\n",
    "        \n",
    "        u = network(tf.concat([x, t], axis=1))\n",
    "        print(\"u:\",u)\n",
    "        print(\"u_shape:\", u.shape)\n",
    "\n",
    "        print(\"x:\", x)\n",
    "        print(\"x_reshape:\", x.shape) # no longer need to reshape after inputting x\n",
    "                                     # and t directly\n",
    "        print(\"t:\", t)\n",
    "        print(\"t_reshape:\", t.shape)\n",
    "\n",
    "\n",
    "        #### still not fit to tensor ####\n",
    "        u_x = tape.gradient(u, x) # cancel: Reshape x to [32,1]\n",
    "        u_t = tape.gradient(u, t) # solved: u_t showed NoneType\n",
    "\n",
    "        print(\"u_x:\", u_x)\n",
    "        print(\"u_t:\", u_t)\n",
    "\n",
    "        # Debugging: Check if any tensors contain NaN values\n",
    "        tf.debugging.assert_all_finite(u_x, \"u_x contains NaN values\")\n",
    "        tf.debugging.assert_all_finite(u_t, \"u_t contains NaN values\")\n",
    "\n",
    "        # Define the Burgers equation residual\n",
    "        residual = u_t + u * u_x - domain.viscosity * tape.gradient(u_x, x)\n",
    "\n",
    "    # Define the boundary and initial condition residuals\n",
    "    # preliminary check\n",
    "    print(\"u:\", u)\n",
    "    initial_residual = u - initial_condition(domain, x)\n",
    "    lower_boundary_residual = u - boundary_conditions(domain, domain.x_min, x, t)\n",
    "    upper_boundary_residual = u - boundary_conditions(domain, domain.x_max, x, t)\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(residual)) + \\\n",
    "           tf.reduce_mean(tf.square(initial_residual)) + \\\n",
    "           tf.reduce_mean(tf.square(lower_boundary_residual)) + \\\n",
    "           tf.reduce_mean(tf.square(upper_boundary_residual))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Define the neural network model\n",
    "def create_network():\n",
    "    network = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(2,)),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(32, activation='tanh'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return network\n",
    "\n",
    "# Train the model using PINNs\n",
    "def train_model(network, domain, num_epochs, learning_rate, batch_size, num_collocation_points):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Generate collocation points\n",
    "    collocation_points_x = tf.random.uniform([num_collocation_points, 1], minval=domain.x_min, maxval=domain.x_max)\n",
    "    collocation_points_t = tf.random.uniform([num_collocation_points, 1], minval=domain.t_min, maxval=domain.t_max)\n",
    "    collocation_points = tf.concat([collocation_points_x, collocation_points_t], axis=1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        x_batch = tf.random.uniform([batch_size, 1], minval=domain.x_min, maxval=domain.x_max)\n",
    "        t_batch = tf.random.uniform([batch_size, 1], minval=domain.t_min, maxval=domain.t_max)\n",
    "        \n",
    "        # Concatenate x_batch and t_batch to create input with shape (batch_size, 2)\n",
    "\n",
    "        # input_batch = tf.concat([x_batch, t_batch], axis=1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = physics_informed_loss(network, domain, x_batch, t_batch, collocation_points)\n",
    "\n",
    "        gradients = tape.gradient(loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, network.trainable_variables))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.numpy()}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Visualize the results\n",
    "def visualize_results(model, domain):\n",
    "    x_grid, t_grid = np.meshgrid(np.linspace(domain.x_min, domain.x_max, domain.num_x),\n",
    "                                 np.linspace(domain.t_min, domain.t_max, domain.num_t))\n",
    "    X = np.hstack((x_grid.flatten()[:, np.newaxis], t_grid.flatten()[:, np.newaxis]))\n",
    "    u_pred_grid = model.predict(X)\n",
    "    u_pred_grid = griddata(X, u_pred_grid.flatten(), (x_grid, t_grid), method='cubic')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.pcolor(x_grid, t_grid, u_pred_grid, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.title('Predicted Velocity')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define domain parameters and create the domain\n",
    "    x_min = -1.0\n",
    "    x_max = 1.0\n",
    "    t_min = 0\n",
    "    t_max = 1.00\n",
    "    num_x = 100\n",
    "    num_t = 100\n",
    "    viscosity = 0.01\n",
    "    domain = Domain(x_min, x_max, t_min, t_max, num_x, num_t, viscosity)\n",
    "\n",
    "    # Create the neural network \n",
    "    network = create_network()\n",
    "\n",
    "    # Train the model using PINNs\n",
    "    model = train_model(network, domain, num_epochs=2000, learning_rate=0.01, batch_size=32, num_collocation_points=1000)\n",
    "\n",
    "    # Visualize the results\n",
    "    visualize_results(model, domain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avvenv",
   "language": "python",
   "name": "avvenv"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
